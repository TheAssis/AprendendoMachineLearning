Hey There!

During the studies, we often need more material than that provided. Then I will write down some material I needed to use an why.

Since there are two classes websites (one for [spring 2015](https://courses.cs.washington.edu/courses/cse446/15sp/) and other for [autumn 2017](https://courses.cs.washington.edu/courses/csep546/17au/)) available, I've had the opportunity to choose what projects to do. I'll explain what I picked up and why.

2. _Decision Trees_
    * Extra Materials
        In order to understand _Information Gain_ I needed a practical example with some visual intuition. Please read Sam T Towards Data Science article: [Entropy: How Decision Trees Make Decisions](https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8)
    
        I've got some trouble trying to understand _Gain Ratio_ as well, but this [J. R. Quinlan Decision Trees and Multi-Valued Attributes](https://aitopics.org/download/classics:BEBDCE7E)) helped me a lot.
    
        The probabilistic method for addressing _missing attribute values_ made me very confusing. The explanation used by Pedro Domingos in the video _8. Learning with Missing Values_ can be found in Mitchell _3.7.4 Handling Training Examples with Missing Attribute Values_. Going there, you're going to find a reference for Quinlan, 1993 (C4.5: Programs for Machine Learning) . The book can be found in ```/books/```. Go to chapter 3 and read Quinlan's explanation.

    * Projects
        For Decision Trees' "homework" I chose the [Letter Recognition project](https://courses.cs.washington.edu/courses/cse446/15sp/assignments/1/hw1.pdf), from [spring 2015 classes](https://courses.cs.washington.edu/courses/cse446/15sp/). I prefer this one because image recognition is something I'm very interested in.
