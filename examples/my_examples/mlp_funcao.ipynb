{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_funcao.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheAssis/AprendendoMachineLearning/blob/master/mlp_funcao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5xAN0S_Qce4G",
        "colab_type": "code",
        "outputId": "5c34724a-14cb-4bb1-e033-eb61c5dbbbe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# importando a base de dados\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import tarfile\n",
        "from six.moves import urllib\n",
        "import csv\n",
        "\n",
        "FILE_TO_DOWNLOAD =  \"data_function.csv\"\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/TheAssis/AprendendoMachineLearning/\"\n",
        "DATA_PATH = \"master/\"\n",
        "DATA_URL = DOWNLOAD_ROOT + DATA_PATH + FILE_TO_DOWNLOAD\n",
        "\n",
        "def fetch_data(data_url=DATA_URL, data_path=DATA_PATH, file_to_download=FILE_TO_DOWNLOAD):\n",
        "  if not os.path.isdir(data_path):\n",
        "    os.makedirs(data_path)\n",
        "  urllib.request.urlretrieve(data_url, data_path+file_to_download)\n",
        "  \n",
        "fetch_data()\n",
        "\n",
        "# observando se o diretório datasets foi criado com sucesso \n",
        "# !ls dados\n",
        "\n",
        "\n",
        "!ls master/\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_function.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dqvDWARnfpwu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#processo de leitura do arquivo de DADOS e separação em entrada e saída\n",
        "\n",
        "\n",
        "leitura = pd.read_csv(open('master/data_function.csv','r')) # converte para um DataFrame or TextParser\n",
        "leitura = np.array(leitura)# e depois converte pra array\n",
        "# as colunas começam e 0 e vão até n-1\n",
        "X = leitura[:,:-1] # dados de entrada (domínio)\n",
        "y = leitura[:,2] # dados de saída\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ro3Fn5xESbUC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# separação em dados de treino e dados de teste\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yBRecO2lSksz",
        "colab_type": "code",
        "outputId": "8a722187-26ae-4d4c-c1f7-1ddd72e69d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2686
        }
      },
      "cell_type": "code",
      "source": [
        "# Part 2 - Now let's make the ANN!\n",
        "\n",
        "# Importing the Scikit libraries and packages\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "\n",
        "# Initialising the ANN (?)\n",
        "mlp_r = MLPRegressor(alpha=0.0001, \n",
        "                     hidden_layer_sizes = (5,5), # two layers of 5 neurons each\n",
        "                     max_iter = 50000,\n",
        "                     activation = 'identity', #identity shows best approach\n",
        "                     verbose = 'False',\n",
        "                     learning_rate = 'adaptive')\n",
        "\n",
        "a = mlp_r.fit(X_train, y_train) # in here the ANN is training, loss at iteration 156 equals 0.00672818 (lesser ever achieved).\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.23056907\n",
            "Iteration 2, loss = 0.21141886\n",
            "Iteration 3, loss = 0.19326178\n",
            "Iteration 4, loss = 0.17640560\n",
            "Iteration 5, loss = 0.16075889\n",
            "Iteration 6, loss = 0.14661236\n",
            "Iteration 7, loss = 0.13360064\n",
            "Iteration 8, loss = 0.12176461\n",
            "Iteration 9, loss = 0.11085813\n",
            "Iteration 10, loss = 0.10132642\n",
            "Iteration 11, loss = 0.09288906\n",
            "Iteration 12, loss = 0.08524674\n",
            "Iteration 13, loss = 0.07861569\n",
            "Iteration 14, loss = 0.07290698\n",
            "Iteration 15, loss = 0.06779046\n",
            "Iteration 16, loss = 0.06356305\n",
            "Iteration 17, loss = 0.05997863\n",
            "Iteration 18, loss = 0.05694173\n",
            "Iteration 19, loss = 0.05451763\n",
            "Iteration 20, loss = 0.05239803\n",
            "Iteration 21, loss = 0.05062637\n",
            "Iteration 22, loss = 0.04931806\n",
            "Iteration 23, loss = 0.04803497\n",
            "Iteration 24, loss = 0.04712898\n",
            "Iteration 25, loss = 0.04626805\n",
            "Iteration 26, loss = 0.04549227\n",
            "Iteration 27, loss = 0.04502706\n",
            "Iteration 28, loss = 0.04443348\n",
            "Iteration 29, loss = 0.04391353\n",
            "Iteration 30, loss = 0.04339651\n",
            "Iteration 31, loss = 0.04294600\n",
            "Iteration 32, loss = 0.04247623\n",
            "Iteration 33, loss = 0.04203663\n",
            "Iteration 34, loss = 0.04157557\n",
            "Iteration 35, loss = 0.04112860\n",
            "Iteration 36, loss = 0.04066890\n",
            "Iteration 37, loss = 0.04023089\n",
            "Iteration 38, loss = 0.03977287\n",
            "Iteration 39, loss = 0.03931733\n",
            "Iteration 40, loss = 0.03887235\n",
            "Iteration 41, loss = 0.03843608\n",
            "Iteration 42, loss = 0.03799896\n",
            "Iteration 43, loss = 0.03756880\n",
            "Iteration 44, loss = 0.03712921\n",
            "Iteration 45, loss = 0.03670736\n",
            "Iteration 46, loss = 0.03627441\n",
            "Iteration 47, loss = 0.03583888\n",
            "Iteration 48, loss = 0.03542689\n",
            "Iteration 49, loss = 0.03499784\n",
            "Iteration 50, loss = 0.03460310\n",
            "Iteration 51, loss = 0.03418196\n",
            "Iteration 52, loss = 0.03375265\n",
            "Iteration 53, loss = 0.03335926\n",
            "Iteration 54, loss = 0.03293970\n",
            "Iteration 55, loss = 0.03254491\n",
            "Iteration 56, loss = 0.03213970\n",
            "Iteration 57, loss = 0.03172419\n",
            "Iteration 58, loss = 0.03133186\n",
            "Iteration 59, loss = 0.03092861\n",
            "Iteration 60, loss = 0.03053918\n",
            "Iteration 61, loss = 0.03013884\n",
            "Iteration 62, loss = 0.02974458\n",
            "Iteration 63, loss = 0.02934957\n",
            "Iteration 64, loss = 0.02896936\n",
            "Iteration 65, loss = 0.02857331\n",
            "Iteration 66, loss = 0.02819565\n",
            "Iteration 67, loss = 0.02781228\n",
            "Iteration 68, loss = 0.02742067\n",
            "Iteration 69, loss = 0.02705952\n",
            "Iteration 70, loss = 0.02668348\n",
            "Iteration 71, loss = 0.02631666\n",
            "Iteration 72, loss = 0.02593810\n",
            "Iteration 73, loss = 0.02557962\n",
            "Iteration 74, loss = 0.02521636\n",
            "Iteration 75, loss = 0.02485784\n",
            "Iteration 76, loss = 0.02449115\n",
            "Iteration 77, loss = 0.02414584\n",
            "Iteration 78, loss = 0.02379007\n",
            "Iteration 79, loss = 0.02344123\n",
            "Iteration 80, loss = 0.02309766\n",
            "Iteration 81, loss = 0.02275697\n",
            "Iteration 82, loss = 0.02241244\n",
            "Iteration 83, loss = 0.02207789\n",
            "Iteration 84, loss = 0.02174336\n",
            "Iteration 85, loss = 0.02141547\n",
            "Iteration 86, loss = 0.02107946\n",
            "Iteration 87, loss = 0.02075809\n",
            "Iteration 88, loss = 0.02043393\n",
            "Iteration 89, loss = 0.02011946\n",
            "Iteration 90, loss = 0.01980735\n",
            "Iteration 91, loss = 0.01949355\n",
            "Iteration 92, loss = 0.01918238\n",
            "Iteration 93, loss = 0.01887328\n",
            "Iteration 94, loss = 0.01857777\n",
            "Iteration 95, loss = 0.01827503\n",
            "Iteration 96, loss = 0.01797934\n",
            "Iteration 97, loss = 0.01769090\n",
            "Iteration 98, loss = 0.01740679\n",
            "Iteration 99, loss = 0.01711440\n",
            "Iteration 100, loss = 0.01683624\n",
            "Iteration 101, loss = 0.01655787\n",
            "Iteration 102, loss = 0.01628412\n",
            "Iteration 103, loss = 0.01601689\n",
            "Iteration 104, loss = 0.01574578\n",
            "Iteration 105, loss = 0.01548351\n",
            "Iteration 106, loss = 0.01521998\n",
            "Iteration 107, loss = 0.01496727\n",
            "Iteration 108, loss = 0.01471410\n",
            "Iteration 109, loss = 0.01446753\n",
            "Iteration 110, loss = 0.01421683\n",
            "Iteration 111, loss = 0.01397912\n",
            "Iteration 112, loss = 0.01373804\n",
            "Iteration 113, loss = 0.01350341\n",
            "Iteration 114, loss = 0.01327700\n",
            "Iteration 115, loss = 0.01304857\n",
            "Iteration 116, loss = 0.01282090\n",
            "Iteration 117, loss = 0.01259955\n",
            "Iteration 118, loss = 0.01238234\n",
            "Iteration 119, loss = 0.01217510\n",
            "Iteration 120, loss = 0.01196302\n",
            "Iteration 121, loss = 0.01175895\n",
            "Iteration 122, loss = 0.01155306\n",
            "Iteration 123, loss = 0.01135771\n",
            "Iteration 124, loss = 0.01116359\n",
            "Iteration 125, loss = 0.01097173\n",
            "Iteration 126, loss = 0.01078589\n",
            "Iteration 127, loss = 0.01060226\n",
            "Iteration 128, loss = 0.01042177\n",
            "Iteration 129, loss = 0.01024346\n",
            "Iteration 130, loss = 0.01007188\n",
            "Iteration 131, loss = 0.00990044\n",
            "Iteration 132, loss = 0.00973760\n",
            "Iteration 133, loss = 0.00957291\n",
            "Iteration 134, loss = 0.00941407\n",
            "Iteration 135, loss = 0.00925473\n",
            "Iteration 136, loss = 0.00910552\n",
            "Iteration 137, loss = 0.00895881\n",
            "Iteration 138, loss = 0.00880962\n",
            "Iteration 139, loss = 0.00866994\n",
            "Iteration 140, loss = 0.00853005\n",
            "Iteration 141, loss = 0.00839390\n",
            "Iteration 142, loss = 0.00826372\n",
            "Iteration 143, loss = 0.00813000\n",
            "Iteration 144, loss = 0.00800374\n",
            "Iteration 145, loss = 0.00788539\n",
            "Iteration 146, loss = 0.00776419\n",
            "Iteration 147, loss = 0.00764835\n",
            "Iteration 148, loss = 0.00753511\n",
            "Iteration 149, loss = 0.00742299\n",
            "Iteration 150, loss = 0.00731489\n",
            "Iteration 151, loss = 0.00721085\n",
            "Iteration 152, loss = 0.00711126\n",
            "Iteration 153, loss = 0.00701096\n",
            "Iteration 154, loss = 0.00691253\n",
            "Iteration 155, loss = 0.00681991\n",
            "Iteration 156, loss = 0.00672818\n",
            "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2VwlDB3e52FV",
        "colab_type": "code",
        "outputId": "54e71d6a-35cb-4c3a-d6e3-6f279050339f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "cell_type": "code",
      "source": [
        "network_prediction = mlp_r.predict(X_test)\n",
        "print(y_test)\n",
        "print('\\n')\n",
        "print(network_prediction)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.42111982 0.72188066 0.45146278 0.04145065 0.65524159 0.0536688\n",
            " 0.56333293 0.13501536 0.35729934 0.67968421 0.79787285 0.86257535\n",
            " 0.88760196 0.12874952 0.4420036  0.09217741 0.2790587  0.80615529\n",
            " 0.65983298 0.28012928 0.79984498 0.46989423 0.46233165 0.02947275\n",
            " 0.79508988 0.2648805  0.73361177 0.15539364 0.13357321 0.98116216\n",
            " 0.06061448 0.35380116 0.98381147 0.49758068 0.94567405 0.68780352\n",
            " 0.41254102 0.23415505 0.09644751 0.01631142 0.49314964 0.48991836\n",
            " 0.41281172 0.64425895 0.56454772 0.12036702 0.75664307 0.33724883\n",
            " 0.99793623 0.1357083  0.1952589  0.57071132 0.57420581 0.43801498\n",
            " 0.77763692 0.24138536 0.60729607 0.35812044 0.97240388 0.25264635\n",
            " 0.99206215 0.919788   0.83824052 0.91062525 0.59505714 0.06182631\n",
            " 0.76792253 0.14028849 0.65823846 0.85269413 0.6140757  0.43315474\n",
            " 0.86981354 0.45920685 0.27211455 0.98695331 0.60945495 0.86578397\n",
            " 0.93329534 0.78795684 0.91803601 0.03705069 0.34713325 0.25432096\n",
            " 0.50411758 0.7061535  0.12302506 0.227471   0.08789055 0.06840693\n",
            " 0.31253546 0.00281316 0.64093923 0.9133482  0.25764092 0.53135667\n",
            " 0.94669173 0.59310865 0.64402715 0.40888469]\n",
            "\n",
            "\n",
            "[ 0.43157633  0.71375996  0.54046296  0.10710243  0.68523385  0.13848854\n",
            "  0.57019105  0.23841032  0.45031428  0.70172858  0.53495033  0.58206075\n",
            "  0.63906404  0.27231732  0.52709459  0.15723109  0.32329986  0.68837341\n",
            "  0.57459377  0.360523    0.64006856  0.5646564   0.54395138  0.09006147\n",
            "  0.60572246  0.33349492  0.61221884  0.30542473  0.19256391  0.79504032\n",
            "  0.15174777  0.46891394  0.85746735  0.41118237  0.71956164  0.65543098\n",
            "  0.47476699  0.38179344  0.14658563  0.04794988  0.59328569  0.58540935\n",
            "  0.51748928  0.6124981   0.55582407  0.24121044  0.74431884  0.42839523\n",
            "  0.87035545  0.19663788  0.29846197  0.48932577  0.64340715  0.51972885\n",
            "  0.73728668  0.36152526  0.66316754  0.48601747  0.78190013  0.29721563\n",
            "  0.6612962   0.55510591  0.79221841  0.6385904   0.51959229  0.1625503\n",
            "  0.75471892  0.26657565  0.6564353   0.79277791  0.48995793  0.35079393\n",
            "  0.78544374  0.52448619  0.35142904  0.74431765  0.57778741  0.71314814\n",
            "  0.8252081   0.6473725   0.82838396  0.10688372  0.42814497  0.37456173\n",
            "  0.5877299   0.71650088  0.24615726  0.28450022  0.18749235  0.1656974\n",
            "  0.45774955 -0.01335908  0.54339413  0.78684483  0.33930884  0.58243707\n",
            "  0.83420563  0.62472934  0.6798007   0.41503964]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zBRtePzoD7JG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e69cc7f-dff5-4788-fe64-22ff3e75bbe6"
      },
      "cell_type": "code",
      "source": [
        "numero_decimal = 3.141592653589793\n",
        "pi =round(numero_decimal, 2)\n",
        "print(pi)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e5TIwoORU5-L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "references: https://www.youtube.com/watch?v=pFm5dvT96rw\n",
        "https://www.youtube.com/watch?v=BCo2c2NJLSk\n",
        "https://www.youtube.com/watch?v=GvUYMKJOJ7E\n",
        "\n",
        "talvez seja melhor usar: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
        "\n",
        "Exemplo comentado de MLPRegressor: http://www.machinelearningtutorial.net/2017/01/28/python-scikit-simple-function-approximation/\n"
      ]
    }
  ]
}